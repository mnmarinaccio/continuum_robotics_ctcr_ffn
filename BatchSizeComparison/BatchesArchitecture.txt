class Model(nn.Module):
    def __init__(self, in_features=numInputs, h1=batch_size, h2=96, h3=50, h4=25, h5=18, h6=15, out_features=numOutputs):
        super().__init__()
        self.fc1 = nn.Linear(in_features, h1)
        self.batchLayer = nn.BatchNorm1d(h1)
        self.layerNorm = LayerNorm(h1)
        self.fc2 = nn.Linear(h1, h2)
        self.fc3 = nn.Linear(h2, h3)
        self.dropout1 = nn.Dropout(0.2)
        self.fc4 = nn.Linear(h3, h4)
        self.fc5 = nn.Linear(h4, h5)
        self.dropout2 = nn.Dropout(0.2)
        self.fc6 = nn.Linear(h5, h6)
        self.out = nn.Linear(h6, out_features)

        # Apply He Initialization to the linear layers
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=0.2)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
        
    def forward(self, x):
        x=self.fc1(x)
        x = self.batchLayer(x)
        x = F.leaky_relu(x)
        x = self.layerNorm(x)
        x=self.fc2(x)
        x = F.leaky_relu(x)
        x=self.fc3(x)
        x = F.leaky_relu(x)
        x = self.dropout1(x)
        x=self.fc4(x)
        x = F.leaky_relu(x)
        x=self.fc5(x)
        x = self.dropout2(x)
        x = F.leaky_relu(x)
        x=self.fc6(x)
        x = F.leaky_relu(x)
        x = self.out(x)
        
        return x