{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8310d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import SmoothL1Loss\n",
    "from torch.nn import L1Loss\n",
    "from torch.nn import LayerNorm\n",
    "import math\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add1919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numInputs = 21\n",
    "numOutputs = 7\n",
    "numEpochs = 150\n",
    "batch_size = 128\n",
    "all_features = 'No'\n",
    "data_splitting = 'Yes'\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "dropout_rates = [0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39db8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.0,in_features=numInputs, h1=batch_size, h2=96, h3=50, h4=30, h5=25, h6=18, h7=15, out_features=numOutputs):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, h1)\n",
    "        self.batchLayer = nn.BatchNorm1d(h1)\n",
    "        self.layerNorm = LayerNorm(h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.fc3 = nn.Linear(h2, h3)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(h3, h4)\n",
    "        self.fc5 = nn.Linear(h4, h5)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc6 = nn.Linear(h5, h6)\n",
    "        self.fc7 = nn.Linear(h6, h7)\n",
    "        self.out = nn.Linear(h7, out_features)\n",
    "\n",
    "        # Apply He Initialization to the linear layers\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=0.2)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.PReLU):\n",
    "                # Initialize PReLU parameters\n",
    "                init.constant_(m.weight, 0.25)  # Set the negative slope to 0.25\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.fc1(x)\n",
    "        x = self.batchLayer(x)\n",
    "        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  \n",
    "        x = self.layerNorm(x)\n",
    "        x=self.fc2(x)\n",
    "        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  \n",
    "        x=self.fc3(x)\n",
    "        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  \n",
    "        x = self.dropout1(x)\n",
    "        x=self.fc4(x)\n",
    "        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device)) \n",
    "        x=self.fc5(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  \n",
    "        x=self.fc6(x)\n",
    "        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  \n",
    "        x=self.fc7(x)\n",
    "        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  \n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b738cd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=21, out_features=128, bias=True)\n",
       "  (batchLayer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc2): Linear(in_features=128, out_features=96, bias=True)\n",
       "  (fc3): Linear(in_features=96, out_features=50, bias=True)\n",
       "  (dropout1): Dropout(p=0.0, inplace=False)\n",
       "  (fc4): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (fc5): Linear(in_features=30, out_features=25, bias=True)\n",
       "  (dropout2): Dropout(p=0.0, inplace=False)\n",
       "  (fc6): Linear(in_features=25, out_features=18, bias=True)\n",
       "  (fc7): Linear(in_features=18, out_features=15, bias=True)\n",
       "  (out): Linear(in_features=15, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(23)\n",
    "model = Model()\n",
    "model.init_weights()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9734db41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>beta1</th>\n",
       "      <th>a2</th>\n",
       "      <th>beta2</th>\n",
       "      <th>a3</th>\n",
       "      <th>beta3</th>\n",
       "      <th>delta_a1</th>\n",
       "      <th>delta_b1</th>\n",
       "      <th>delta_a2</th>\n",
       "      <th>delta_b2</th>\n",
       "      <th>...</th>\n",
       "      <th>sigma_21</th>\n",
       "      <th>sigma_22</th>\n",
       "      <th>sigma_23</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>z3</th>\n",
       "      <th>n3</th>\n",
       "      <th>sigma_31</th>\n",
       "      <th>sigma_32</th>\n",
       "      <th>sigma_33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.038446</td>\n",
       "      <td>-8.648766</td>\n",
       "      <td>-0.934423</td>\n",
       "      <td>-4.659318</td>\n",
       "      <td>-0.518523</td>\n",
       "      <td>-3.419294</td>\n",
       "      <td>-1.038446</td>\n",
       "      <td>-8.648766</td>\n",
       "      <td>-0.934423</td>\n",
       "      <td>-4.659318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086870</td>\n",
       "      <td>-0.409851</td>\n",
       "      <td>0.548841</td>\n",
       "      <td>-51.311665</td>\n",
       "      <td>-52.394960</td>\n",
       "      <td>-196.172089</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.402930</td>\n",
       "      <td>-0.085287</td>\n",
       "      <td>0.636759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.849078</td>\n",
       "      <td>-12.935190</td>\n",
       "      <td>-0.948167</td>\n",
       "      <td>-4.141172</td>\n",
       "      <td>-0.835220</td>\n",
       "      <td>-1.755485</td>\n",
       "      <td>0.189368</td>\n",
       "      <td>-4.286425</td>\n",
       "      <td>-0.013744</td>\n",
       "      <td>0.518146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122501</td>\n",
       "      <td>-0.453856</td>\n",
       "      <td>0.544632</td>\n",
       "      <td>-50.692317</td>\n",
       "      <td>-60.809199</td>\n",
       "      <td>-208.382272</td>\n",
       "      <td>0.598864</td>\n",
       "      <td>0.371977</td>\n",
       "      <td>-0.123544</td>\n",
       "      <td>0.698378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.017749</td>\n",
       "      <td>-12.567895</td>\n",
       "      <td>-0.812003</td>\n",
       "      <td>-12.013610</td>\n",
       "      <td>-0.812547</td>\n",
       "      <td>-8.885584</td>\n",
       "      <td>-0.168671</td>\n",
       "      <td>0.367295</td>\n",
       "      <td>0.136164</td>\n",
       "      <td>-7.872439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095631</td>\n",
       "      <td>-0.442791</td>\n",
       "      <td>0.565367</td>\n",
       "      <td>-45.198441</td>\n",
       "      <td>-62.608553</td>\n",
       "      <td>-201.395252</td>\n",
       "      <td>0.611581</td>\n",
       "      <td>0.389910</td>\n",
       "      <td>-0.085298</td>\n",
       "      <td>0.683128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.549890</td>\n",
       "      <td>-12.088090</td>\n",
       "      <td>-0.942784</td>\n",
       "      <td>-10.099092</td>\n",
       "      <td>-0.983485</td>\n",
       "      <td>-6.809770</td>\n",
       "      <td>0.467859</td>\n",
       "      <td>0.479805</td>\n",
       "      <td>-0.130781</td>\n",
       "      <td>1.914518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136366</td>\n",
       "      <td>-0.478341</td>\n",
       "      <td>0.542060</td>\n",
       "      <td>-47.439516</td>\n",
       "      <td>-65.997012</td>\n",
       "      <td>-207.153421</td>\n",
       "      <td>0.527576</td>\n",
       "      <td>0.289144</td>\n",
       "      <td>-0.097064</td>\n",
       "      <td>0.792867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.983188</td>\n",
       "      <td>-22.793282</td>\n",
       "      <td>-0.975256</td>\n",
       "      <td>-3.773322</td>\n",
       "      <td>-0.944051</td>\n",
       "      <td>-1.200367</td>\n",
       "      <td>-0.433298</td>\n",
       "      <td>-10.705192</td>\n",
       "      <td>-0.032472</td>\n",
       "      <td>6.325771</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155923</td>\n",
       "      <td>-0.473531</td>\n",
       "      <td>0.541701</td>\n",
       "      <td>-51.306577</td>\n",
       "      <td>-58.613165</td>\n",
       "      <td>-219.831348</td>\n",
       "      <td>0.616890</td>\n",
       "      <td>0.331818</td>\n",
       "      <td>-0.194669</td>\n",
       "      <td>0.686621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1.013616</td>\n",
       "      <td>-115.908578</td>\n",
       "      <td>0.129843</td>\n",
       "      <td>-86.372926</td>\n",
       "      <td>0.793488</td>\n",
       "      <td>-47.812503</td>\n",
       "      <td>0.660221</td>\n",
       "      <td>3.220634</td>\n",
       "      <td>0.054423</td>\n",
       "      <td>4.742424</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.465600</td>\n",
       "      <td>-0.127947</td>\n",
       "      <td>0.686238</td>\n",
       "      <td>11.500513</td>\n",
       "      <td>28.227900</td>\n",
       "      <td>-245.757880</td>\n",
       "      <td>0.634882</td>\n",
       "      <td>-0.383059</td>\n",
       "      <td>0.067531</td>\n",
       "      <td>0.667555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.991986</td>\n",
       "      <td>-116.383328</td>\n",
       "      <td>0.789818</td>\n",
       "      <td>-88.284428</td>\n",
       "      <td>0.139324</td>\n",
       "      <td>-44.712237</td>\n",
       "      <td>-0.021630</td>\n",
       "      <td>-0.474749</td>\n",
       "      <td>0.659975</td>\n",
       "      <td>-1.911502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.425068</td>\n",
       "      <td>-0.162333</td>\n",
       "      <td>0.736364</td>\n",
       "      <td>10.706496</td>\n",
       "      <td>17.988644</td>\n",
       "      <td>-240.644806</td>\n",
       "      <td>0.604545</td>\n",
       "      <td>-0.346952</td>\n",
       "      <td>0.039395</td>\n",
       "      <td>0.715960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.987575</td>\n",
       "      <td>-113.233227</td>\n",
       "      <td>0.737640</td>\n",
       "      <td>-82.726911</td>\n",
       "      <td>0.921319</td>\n",
       "      <td>-42.586561</td>\n",
       "      <td>-0.004412</td>\n",
       "      <td>3.150100</td>\n",
       "      <td>-0.052177</td>\n",
       "      <td>5.557516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.522901</td>\n",
       "      <td>-0.001744</td>\n",
       "      <td>0.723779</td>\n",
       "      <td>9.452766</td>\n",
       "      <td>35.094901</td>\n",
       "      <td>-250.906354</td>\n",
       "      <td>0.580659</td>\n",
       "      <td>-0.459221</td>\n",
       "      <td>0.149290</td>\n",
       "      <td>0.655487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0.881862</td>\n",
       "      <td>-117.309543</td>\n",
       "      <td>0.918263</td>\n",
       "      <td>-85.435163</td>\n",
       "      <td>0.594123</td>\n",
       "      <td>-48.462650</td>\n",
       "      <td>-0.105713</td>\n",
       "      <td>-4.076316</td>\n",
       "      <td>0.180623</td>\n",
       "      <td>-2.708252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482495</td>\n",
       "      <td>-0.034416</td>\n",
       "      <td>0.741441</td>\n",
       "      <td>12.219248</td>\n",
       "      <td>28.893432</td>\n",
       "      <td>-247.017541</td>\n",
       "      <td>0.576371</td>\n",
       "      <td>-0.444613</td>\n",
       "      <td>0.094256</td>\n",
       "      <td>0.679140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0.618577</td>\n",
       "      <td>-123.498620</td>\n",
       "      <td>0.502818</td>\n",
       "      <td>-94.074584</td>\n",
       "      <td>0.843519</td>\n",
       "      <td>-49.858772</td>\n",
       "      <td>-0.263285</td>\n",
       "      <td>-6.189077</td>\n",
       "      <td>-0.415445</td>\n",
       "      <td>-8.639421</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.519509</td>\n",
       "      <td>-0.081435</td>\n",
       "      <td>0.714351</td>\n",
       "      <td>16.364599</td>\n",
       "      <td>27.650480</td>\n",
       "      <td>-251.611725</td>\n",
       "      <td>0.616123</td>\n",
       "      <td>-0.426460</td>\n",
       "      <td>0.075815</td>\n",
       "      <td>0.657858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             a1       beta1        a2      beta2        a3      beta3  \\\n",
       "0     -1.038446   -8.648766 -0.934423  -4.659318 -0.518523  -3.419294   \n",
       "1     -0.849078  -12.935190 -0.948167  -4.141172 -0.835220  -1.755485   \n",
       "2     -1.017749  -12.567895 -0.812003 -12.013610 -0.812547  -8.885584   \n",
       "3     -0.549890  -12.088090 -0.942784 -10.099092 -0.983485  -6.809770   \n",
       "4     -0.983188  -22.793282 -0.975256  -3.773322 -0.944051  -1.200367   \n",
       "...         ...         ...       ...        ...       ...        ...   \n",
       "99995  1.013616 -115.908578  0.129843 -86.372926  0.793488 -47.812503   \n",
       "99996  0.991986 -116.383328  0.789818 -88.284428  0.139324 -44.712237   \n",
       "99997  0.987575 -113.233227  0.737640 -82.726911  0.921319 -42.586561   \n",
       "99998  0.881862 -117.309543  0.918263 -85.435163  0.594123 -48.462650   \n",
       "99999  0.618577 -123.498620  0.502818 -94.074584  0.843519 -49.858772   \n",
       "\n",
       "       delta_a1   delta_b1  delta_a2  delta_b2  ...  sigma_21  sigma_22  \\\n",
       "0     -1.038446  -8.648766 -0.934423 -4.659318  ...  0.086870 -0.409851   \n",
       "1      0.189368  -4.286425 -0.013744  0.518146  ...  0.122501 -0.453856   \n",
       "2     -0.168671   0.367295  0.136164 -7.872439  ...  0.095631 -0.442791   \n",
       "3      0.467859   0.479805 -0.130781  1.914518  ...  0.136366 -0.478341   \n",
       "4     -0.433298 -10.705192 -0.032472  6.325771  ...  0.155923 -0.473531   \n",
       "...         ...        ...       ...       ...  ...       ...       ...   \n",
       "99995  0.660221   3.220634  0.054423  4.742424  ... -0.465600 -0.127947   \n",
       "99996 -0.021630  -0.474749  0.659975 -1.911502  ... -0.425068 -0.162333   \n",
       "99997 -0.004412   3.150100 -0.052177  5.557516  ... -0.522901 -0.001744   \n",
       "99998 -0.105713  -4.076316  0.180623 -2.708252  ... -0.482495 -0.034416   \n",
       "99999 -0.263285  -6.189077 -0.415445 -8.639421  ... -0.519509 -0.081435   \n",
       "\n",
       "       sigma_23         x3         y3          z3        n3  sigma_31  \\\n",
       "0      0.548841 -51.311665 -52.394960 -196.172089  0.651852  0.402930   \n",
       "1      0.544632 -50.692317 -60.809199 -208.382272  0.598864  0.371977   \n",
       "2      0.565367 -45.198441 -62.608553 -201.395252  0.611581  0.389910   \n",
       "3      0.542060 -47.439516 -65.997012 -207.153421  0.527576  0.289144   \n",
       "4      0.541701 -51.306577 -58.613165 -219.831348  0.616890  0.331818   \n",
       "...         ...        ...        ...         ...       ...       ...   \n",
       "99995  0.686238  11.500513  28.227900 -245.757880  0.634882 -0.383059   \n",
       "99996  0.736364  10.706496  17.988644 -240.644806  0.604545 -0.346952   \n",
       "99997  0.723779   9.452766  35.094901 -250.906354  0.580659 -0.459221   \n",
       "99998  0.741441  12.219248  28.893432 -247.017541  0.576371 -0.444613   \n",
       "99999  0.714351  16.364599  27.650480 -251.611725  0.616123 -0.426460   \n",
       "\n",
       "       sigma_32  sigma_33  \n",
       "0     -0.085287  0.636759  \n",
       "1     -0.123544  0.698378  \n",
       "2     -0.085298  0.683128  \n",
       "3     -0.097064  0.792867  \n",
       "4     -0.194669  0.686621  \n",
       "...         ...       ...  \n",
       "99995  0.067531  0.667555  \n",
       "99996  0.039395  0.715960  \n",
       "99997  0.149290  0.655487  \n",
       "99998  0.094256  0.679140  \n",
       "99999  0.075815  0.657858  \n",
       "\n",
       "[100000 rows x 40 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robot_df = pd.read_csv('./data/CRL-Dataset-CTCR-Pose.csv')\n",
    "robot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13533657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>z0</th>\n",
       "      <th>n0</th>\n",
       "      <th>sigma_01</th>\n",
       "      <th>sigma_02</th>\n",
       "      <th>sigma_03</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>...</th>\n",
       "      <th>sigma_21</th>\n",
       "      <th>sigma_22</th>\n",
       "      <th>sigma_23</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>z3</th>\n",
       "      <th>n3</th>\n",
       "      <th>sigma_31</th>\n",
       "      <th>sigma_32</th>\n",
       "      <th>sigma_33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.280431</td>\n",
       "      <td>4.081318</td>\n",
       "      <td>-275.017175</td>\n",
       "      <td>0.489505</td>\n",
       "      <td>-0.453508</td>\n",
       "      <td>-0.511684</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>-12.460487</td>\n",
       "      <td>-4.449731</td>\n",
       "      <td>-255.537662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086870</td>\n",
       "      <td>-0.409851</td>\n",
       "      <td>0.548841</td>\n",
       "      <td>-51.311665</td>\n",
       "      <td>-52.394960</td>\n",
       "      <td>-196.172089</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.402930</td>\n",
       "      <td>-0.085287</td>\n",
       "      <td>0.636759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.288158</td>\n",
       "      <td>4.093814</td>\n",
       "      <td>-275.020874</td>\n",
       "      <td>0.495786</td>\n",
       "      <td>-0.450498</td>\n",
       "      <td>-0.509697</td>\n",
       "      <td>0.539867</td>\n",
       "      <td>-13.899759</td>\n",
       "      <td>-9.557922</td>\n",
       "      <td>-258.578839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122501</td>\n",
       "      <td>-0.453856</td>\n",
       "      <td>0.544632</td>\n",
       "      <td>-50.692317</td>\n",
       "      <td>-60.809199</td>\n",
       "      <td>-208.382272</td>\n",
       "      <td>0.598864</td>\n",
       "      <td>0.371977</td>\n",
       "      <td>-0.123544</td>\n",
       "      <td>0.698378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.356485</td>\n",
       "      <td>4.118624</td>\n",
       "      <td>-275.008368</td>\n",
       "      <td>0.485806</td>\n",
       "      <td>-0.453301</td>\n",
       "      <td>-0.514025</td>\n",
       "      <td>0.542484</td>\n",
       "      <td>-7.301657</td>\n",
       "      <td>-9.559360</td>\n",
       "      <td>-258.356336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095631</td>\n",
       "      <td>-0.442791</td>\n",
       "      <td>0.565367</td>\n",
       "      <td>-45.198441</td>\n",
       "      <td>-62.608553</td>\n",
       "      <td>-201.395252</td>\n",
       "      <td>0.611581</td>\n",
       "      <td>0.389910</td>\n",
       "      <td>-0.085298</td>\n",
       "      <td>0.683128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92.348701</td>\n",
       "      <td>4.052783</td>\n",
       "      <td>-275.007550</td>\n",
       "      <td>0.484357</td>\n",
       "      <td>-0.453081</td>\n",
       "      <td>-0.512936</td>\n",
       "      <td>0.544989</td>\n",
       "      <td>-8.899060</td>\n",
       "      <td>-11.335220</td>\n",
       "      <td>-260.598053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136366</td>\n",
       "      <td>-0.478341</td>\n",
       "      <td>0.542060</td>\n",
       "      <td>-47.439516</td>\n",
       "      <td>-65.997012</td>\n",
       "      <td>-207.153421</td>\n",
       "      <td>0.527576</td>\n",
       "      <td>0.289144</td>\n",
       "      <td>-0.097064</td>\n",
       "      <td>0.792867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92.294434</td>\n",
       "      <td>4.048764</td>\n",
       "      <td>-275.027246</td>\n",
       "      <td>0.480633</td>\n",
       "      <td>-0.446826</td>\n",
       "      <td>-0.516235</td>\n",
       "      <td>0.550310</td>\n",
       "      <td>-14.529558</td>\n",
       "      <td>-11.010974</td>\n",
       "      <td>-260.296765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155923</td>\n",
       "      <td>-0.473531</td>\n",
       "      <td>0.541701</td>\n",
       "      <td>-51.306577</td>\n",
       "      <td>-58.613165</td>\n",
       "      <td>-219.831348</td>\n",
       "      <td>0.616890</td>\n",
       "      <td>0.331818</td>\n",
       "      <td>-0.194669</td>\n",
       "      <td>0.686621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>92.497295</td>\n",
       "      <td>4.024072</td>\n",
       "      <td>-274.929230</td>\n",
       "      <td>0.483441</td>\n",
       "      <td>-0.465066</td>\n",
       "      <td>-0.515739</td>\n",
       "      <td>0.532928</td>\n",
       "      <td>32.677843</td>\n",
       "      <td>14.391295</td>\n",
       "      <td>-262.080109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.465600</td>\n",
       "      <td>-0.127947</td>\n",
       "      <td>0.686238</td>\n",
       "      <td>11.500513</td>\n",
       "      <td>28.227900</td>\n",
       "      <td>-245.757880</td>\n",
       "      <td>0.634882</td>\n",
       "      <td>-0.383059</td>\n",
       "      <td>0.067531</td>\n",
       "      <td>0.667555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>92.496686</td>\n",
       "      <td>4.023101</td>\n",
       "      <td>-274.907916</td>\n",
       "      <td>0.488755</td>\n",
       "      <td>-0.460660</td>\n",
       "      <td>-0.511267</td>\n",
       "      <td>0.536206</td>\n",
       "      <td>29.669851</td>\n",
       "      <td>9.457436</td>\n",
       "      <td>-258.303601</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.425068</td>\n",
       "      <td>-0.162333</td>\n",
       "      <td>0.736364</td>\n",
       "      <td>10.706496</td>\n",
       "      <td>17.988644</td>\n",
       "      <td>-240.644806</td>\n",
       "      <td>0.604545</td>\n",
       "      <td>-0.346952</td>\n",
       "      <td>0.039395</td>\n",
       "      <td>0.715960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>92.516028</td>\n",
       "      <td>4.027790</td>\n",
       "      <td>-274.921442</td>\n",
       "      <td>0.480131</td>\n",
       "      <td>-0.451220</td>\n",
       "      <td>-0.520775</td>\n",
       "      <td>0.542834</td>\n",
       "      <td>28.468763</td>\n",
       "      <td>18.413199</td>\n",
       "      <td>-262.917468</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.522901</td>\n",
       "      <td>-0.001744</td>\n",
       "      <td>0.723779</td>\n",
       "      <td>9.452766</td>\n",
       "      <td>35.094901</td>\n",
       "      <td>-250.906354</td>\n",
       "      <td>0.580659</td>\n",
       "      <td>-0.459221</td>\n",
       "      <td>0.149290</td>\n",
       "      <td>0.655487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>92.502037</td>\n",
       "      <td>4.016432</td>\n",
       "      <td>-274.878772</td>\n",
       "      <td>0.484499</td>\n",
       "      <td>-0.455910</td>\n",
       "      <td>-0.517875</td>\n",
       "      <td>0.537785</td>\n",
       "      <td>32.681670</td>\n",
       "      <td>14.343887</td>\n",
       "      <td>-261.432513</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482495</td>\n",
       "      <td>-0.034416</td>\n",
       "      <td>0.741441</td>\n",
       "      <td>12.219248</td>\n",
       "      <td>28.893432</td>\n",
       "      <td>-247.017541</td>\n",
       "      <td>0.576371</td>\n",
       "      <td>-0.444613</td>\n",
       "      <td>0.094256</td>\n",
       "      <td>0.679140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>92.507069</td>\n",
       "      <td>4.017249</td>\n",
       "      <td>-274.905774</td>\n",
       "      <td>0.485053</td>\n",
       "      <td>-0.456476</td>\n",
       "      <td>-0.515609</td>\n",
       "      <td>0.538981</td>\n",
       "      <td>34.342705</td>\n",
       "      <td>15.411196</td>\n",
       "      <td>-263.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.519509</td>\n",
       "      <td>-0.081435</td>\n",
       "      <td>0.714351</td>\n",
       "      <td>16.364599</td>\n",
       "      <td>27.650480</td>\n",
       "      <td>-251.611725</td>\n",
       "      <td>0.616123</td>\n",
       "      <td>-0.426460</td>\n",
       "      <td>0.075815</td>\n",
       "      <td>0.657858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x0        y0          z0        n0  sigma_01  sigma_02  \\\n",
       "0      92.280431  4.081318 -275.017175  0.489505 -0.453508 -0.511684   \n",
       "1      92.288158  4.093814 -275.020874  0.495786 -0.450498 -0.509697   \n",
       "2      92.356485  4.118624 -275.008368  0.485806 -0.453301 -0.514025   \n",
       "3      92.348701  4.052783 -275.007550  0.484357 -0.453081 -0.512936   \n",
       "4      92.294434  4.048764 -275.027246  0.480633 -0.446826 -0.516235   \n",
       "...          ...       ...         ...       ...       ...       ...   \n",
       "99995  92.497295  4.024072 -274.929230  0.483441 -0.465066 -0.515739   \n",
       "99996  92.496686  4.023101 -274.907916  0.488755 -0.460660 -0.511267   \n",
       "99997  92.516028  4.027790 -274.921442  0.480131 -0.451220 -0.520775   \n",
       "99998  92.502037  4.016432 -274.878772  0.484499 -0.455910 -0.517875   \n",
       "99999  92.507069  4.017249 -274.905774  0.485053 -0.456476 -0.515609   \n",
       "\n",
       "       sigma_03         x1         y1          z1  ...  sigma_21  sigma_22  \\\n",
       "0      0.541197 -12.460487  -4.449731 -255.537662  ...  0.086870 -0.409851   \n",
       "1      0.539867 -13.899759  -9.557922 -258.578839  ...  0.122501 -0.453856   \n",
       "2      0.542484  -7.301657  -9.559360 -258.356336  ...  0.095631 -0.442791   \n",
       "3      0.544989  -8.899060 -11.335220 -260.598053  ...  0.136366 -0.478341   \n",
       "4      0.550310 -14.529558 -11.010974 -260.296765  ...  0.155923 -0.473531   \n",
       "...         ...        ...        ...         ...  ...       ...       ...   \n",
       "99995  0.532928  32.677843  14.391295 -262.080109  ... -0.465600 -0.127947   \n",
       "99996  0.536206  29.669851   9.457436 -258.303601  ... -0.425068 -0.162333   \n",
       "99997  0.542834  28.468763  18.413199 -262.917468  ... -0.522901 -0.001744   \n",
       "99998  0.537785  32.681670  14.343887 -261.432513  ... -0.482495 -0.034416   \n",
       "99999  0.538981  34.342705  15.411196 -263.000000  ... -0.519509 -0.081435   \n",
       "\n",
       "       sigma_23         x3         y3          z3        n3  sigma_31  \\\n",
       "0      0.548841 -51.311665 -52.394960 -196.172089  0.651852  0.402930   \n",
       "1      0.544632 -50.692317 -60.809199 -208.382272  0.598864  0.371977   \n",
       "2      0.565367 -45.198441 -62.608553 -201.395252  0.611581  0.389910   \n",
       "3      0.542060 -47.439516 -65.997012 -207.153421  0.527576  0.289144   \n",
       "4      0.541701 -51.306577 -58.613165 -219.831348  0.616890  0.331818   \n",
       "...         ...        ...        ...         ...       ...       ...   \n",
       "99995  0.686238  11.500513  28.227900 -245.757880  0.634882 -0.383059   \n",
       "99996  0.736364  10.706496  17.988644 -240.644806  0.604545 -0.346952   \n",
       "99997  0.723779   9.452766  35.094901 -250.906354  0.580659 -0.459221   \n",
       "99998  0.741441  12.219248  28.893432 -247.017541  0.576371 -0.444613   \n",
       "99999  0.714351  16.364599  27.650480 -251.611725  0.616123 -0.426460   \n",
       "\n",
       "       sigma_32  sigma_33  \n",
       "0     -0.085287  0.636759  \n",
       "1     -0.123544  0.698378  \n",
       "2     -0.085298  0.683128  \n",
       "3     -0.097064  0.792867  \n",
       "4     -0.194669  0.686621  \n",
       "...         ...       ...  \n",
       "99995  0.067531  0.667555  \n",
       "99996  0.039395  0.715960  \n",
       "99997  0.149290  0.655487  \n",
       "99998  0.094256  0.679140  \n",
       "99999  0.075815  0.657858  \n",
       "\n",
       "[100000 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if all_features == 'No':\n",
    "    drop_features = ['a1', 'beta1', 'a2', 'beta2', 'a3', 'beta3', 'delta_a1', 'delta_b1', 'delta_a2', 'delta_b2', \n",
    "                 'delta_a3', 'delta_b3']\n",
    "    end_select = 21\n",
    "elif all_features == 'Yes':\n",
    "    drop_features = []\n",
    "    end_select = 33\n",
    "else:\n",
    "    print(\"INVALID SYNTAX, all_features should be Yes or No\")\n",
    "robot_df_final = robot_df.drop(drop_features, axis=\"columns\")\n",
    "robot_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb1c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = robot_df_final.iloc[:, :end_select]  # Selecting the first 21 columns\n",
    "y = robot_df_final.iloc[:, -7:]  # Selecting the last 7 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac70a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy arrays\n",
    "x_np = x.to_numpy()\n",
    "y_np = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b78e1299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Sample Indices: [ 5503  3003  4287 ... 89933 98955 87549]; Shape: (80000,)\n",
      "\n",
      "Validation Sample Indices: [ 7432  3226  2356 ... 90049 96239 91502]; Shape: (10000,)\n",
      "\n",
      "Test Sample Indices: [  447  5318  8604 ... 87990 90793 90717]; Shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "if data_splitting == 'No':\n",
    "    #split into training, validation, and testing set\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(x_np, y_np, test_size=0.2, random_state=23)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=23)\n",
    "elif data_splitting == 'Yes':\n",
    "    #Split evenly based on dataset length \n",
    "    total_rows = x_np.shape[0]\n",
    "    section_size = 12500\n",
    "    num_sections = total_rows // section_size\n",
    "    ind_list = []\n",
    "    train_sample_inds_list = []\n",
    "    val_sample_inds_list = []\n",
    "    test_sample_inds_list = []\n",
    "\n",
    "    start_ind = 0\n",
    "    section_end = start_ind + section_size\n",
    "    for i in range(start_ind,total_rows,section_size):\n",
    "        ind_list.append(np.arange(i, section_end))\n",
    "        section_end += section_size\n",
    "\n",
    "    # Create an array with section indices\n",
    "    sections = np.arange(num_sections)\n",
    "\n",
    "    # Shuffle the sections\n",
    "    np.random.shuffle(sections)\n",
    "\n",
    "    # Calculate the number of rows for each subset\n",
    "    train_num_points = 80000 // num_sections\n",
    "    val_num_points = 10000 // num_sections\n",
    "    #val_test_size = (total_rows - train_size) // 2\n",
    "\n",
    "    for i in range(len(ind_list)):\n",
    "        # Generate random indices for the first sample\n",
    "        train_sample_inds = np.random.choice(ind_list[i], size=train_num_points, replace=False)\n",
    "\n",
    "        # Generate indices for the second sample without overlap\n",
    "        remaining_inds1 = np.setdiff1d(ind_list[i], train_sample_inds)\n",
    "        val_sample_inds = np.random.choice(remaining_inds1, size=val_num_points, replace=False)\n",
    "\n",
    "        remaining_inds2 = np.setdiff1d(remaining_inds1, val_sample_inds)\n",
    "        test_sample_inds = np.random.choice(remaining_inds2, size=val_num_points, replace=False)\n",
    "\n",
    "        train_sample_inds_list.append(train_sample_inds)\n",
    "        val_sample_inds_list.append(val_sample_inds)\n",
    "        test_sample_inds_list.append(test_sample_inds) \n",
    "\n",
    "    train_indices = np.concatenate(train_sample_inds_list)\n",
    "    val_indices = np.concatenate(val_sample_inds_list)\n",
    "    test_indices = np.concatenate(test_sample_inds_list)\n",
    "\n",
    "    X_train, y_train = x_np[train_indices, :], y_np[train_indices, :]\n",
    "    X_val, y_val = x_np[val_indices, :], y_np[val_indices, :]\n",
    "    X_test, y_test = x_np[test_indices, :], y_np[test_indices, :]\n",
    "\n",
    "    print(f'Train Sample Indices: {train_indices}; Shape: {train_indices.shape}')\n",
    "    print(\"\")\n",
    "    print(f'Validation Sample Indices: {val_indices}; Shape: {val_indices.shape}')\n",
    "    print(\"\")\n",
    "    print(f'Test Sample Indices: {test_indices}; Shape: {test_indices.shape}')\n",
    "else:\n",
    "    print(\"INVALID SYNTAX, data_splitting should be Yes or No\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e479ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_val = torch.FloatTensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3feec4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = nn.L1Loss()\n",
    "#criterion = nn.HuberLoss()\n",
    "criterion = SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "730ff2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate = 0.1; Dropout Rate = 0.2; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 5.922486316108704; Average Validation Loss: 4.138407456723949\n",
      "Epoch 10 - Average Training Loss: 24463.025508007813; Average Validation Loss: 82816.60452927215\n",
      "Epoch 20 - Average Training Loss: 10075.010949414063; Average Validation Loss: 22698.722507911392\n",
      "Epoch 30 - Average Training Loss: 521.6159972839356; Average Validation Loss: 52.708351618126976\n",
      "Epoch 40 - Average Training Loss: 53122.6444796875; Average Validation Loss: 437794.19818037975\n",
      "Epoch 50 - Average Training Loss: 15161.722952539063; Average Validation Loss: 26877.08047369462\n",
      "Epoch 60 - Average Training Loss: 2318.119933203125; Average Validation Loss: 272.6395680874209\n",
      "Epoch 70 - Average Training Loss: 179.19639259643554; Average Validation Loss: 104.03220995166633\n",
      "Epoch 80 - Average Training Loss: 53924.9776234375; Average Validation Loss: 3944.0511242830303\n",
      "Epoch 90 - Average Training Loss: 11889.38573828125; Average Validation Loss: 9445.76426522943\n",
      "Epoch 100 - Average Training Loss: 47646305.221635155; Average Validation Loss: 146791.7654272152\n",
      "Epoch 110 - Average Training Loss: 21556.023881640624; Average Validation Loss: 2430.7582575158226\n",
      "Epoch 120 - Average Training Loss: 30793.64306484375; Average Validation Loss: 96970.5233386076\n",
      "Epoch 130 - Average Training Loss: 2248200.120275; Average Validation Loss: 1575971.805379747\n",
      "Epoch 140 - Average Training Loss: 22561.00885078125; Average Validation Loss: 10000.737143987342\n",
      "Epoch 149 - Average Training Loss: 5644.406424804687; Average Validation Loss: 4965.892559582674\n",
      "Learning Rate = 0.1; Dropout Rate = 0.3; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 5.922486316108704; Average Validation Loss: 4.138407456723949\n",
      "Epoch 10 - Average Training Loss: 24463.025508007813; Average Validation Loss: 82816.60452927215\n",
      "Epoch 20 - Average Training Loss: 10075.010949414063; Average Validation Loss: 22698.722507911392\n",
      "Epoch 30 - Average Training Loss: 521.6159972839356; Average Validation Loss: 52.708351618126976\n",
      "Epoch 40 - Average Training Loss: 53122.6444796875; Average Validation Loss: 437794.19818037975\n",
      "Epoch 50 - Average Training Loss: 15161.722952539063; Average Validation Loss: 26877.08047369462\n",
      "Epoch 60 - Average Training Loss: 2318.119933203125; Average Validation Loss: 272.6395680874209\n",
      "Epoch 70 - Average Training Loss: 179.19639259643554; Average Validation Loss: 104.03220995166633\n",
      "Epoch 80 - Average Training Loss: 53924.9776234375; Average Validation Loss: 3944.0511242830303\n",
      "Epoch 90 - Average Training Loss: 11889.38573828125; Average Validation Loss: 9445.76426522943\n",
      "Epoch 100 - Average Training Loss: 47646305.221635155; Average Validation Loss: 146791.7654272152\n",
      "Epoch 110 - Average Training Loss: 21556.023881640624; Average Validation Loss: 2430.7582575158226\n",
      "Epoch 120 - Average Training Loss: 30793.64306484375; Average Validation Loss: 96970.5233386076\n",
      "Epoch 130 - Average Training Loss: 2248200.120275; Average Validation Loss: 1575971.805379747\n",
      "Epoch 140 - Average Training Loss: 22561.00885078125; Average Validation Loss: 10000.737143987342\n",
      "Epoch 149 - Average Training Loss: 5644.406424804687; Average Validation Loss: 4965.892559582674\n",
      "Learning Rate = 0.1; Dropout Rate = 0.4; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 5.922486316108704; Average Validation Loss: 4.138407456723949\n",
      "Epoch 10 - Average Training Loss: 24463.025508007813; Average Validation Loss: 82816.60452927215\n",
      "Epoch 20 - Average Training Loss: 10075.010949414063; Average Validation Loss: 22698.722507911392\n",
      "Epoch 30 - Average Training Loss: 521.6159972839356; Average Validation Loss: 52.708351618126976\n",
      "Epoch 40 - Average Training Loss: 53122.6444796875; Average Validation Loss: 437794.19818037975\n",
      "Epoch 50 - Average Training Loss: 15161.722952539063; Average Validation Loss: 26877.08047369462\n",
      "Epoch 60 - Average Training Loss: 2318.119933203125; Average Validation Loss: 272.6395680874209\n",
      "Epoch 70 - Average Training Loss: 179.19639259643554; Average Validation Loss: 104.03220995166633\n",
      "Epoch 80 - Average Training Loss: 53924.9776234375; Average Validation Loss: 3944.0511242830303\n",
      "Epoch 90 - Average Training Loss: 11889.38573828125; Average Validation Loss: 9445.76426522943\n",
      "Epoch 100 - Average Training Loss: 47646305.221635155; Average Validation Loss: 146791.7654272152\n",
      "Epoch 110 - Average Training Loss: 21556.023881640624; Average Validation Loss: 2430.7582575158226\n",
      "Epoch 120 - Average Training Loss: 30793.64306484375; Average Validation Loss: 96970.5233386076\n",
      "Epoch 130 - Average Training Loss: 2248200.120275; Average Validation Loss: 1575971.805379747\n",
      "Epoch 140 - Average Training Loss: 22561.00885078125; Average Validation Loss: 10000.737143987342\n",
      "Epoch 149 - Average Training Loss: 5644.406424804687; Average Validation Loss: 4965.892559582674\n",
      "Learning Rate = 0.1; Dropout Rate = 0.5; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 5.922486316108704; Average Validation Loss: 4.138407456723949\n",
      "Epoch 10 - Average Training Loss: 24463.025508007813; Average Validation Loss: 82816.60452927215\n",
      "Epoch 20 - Average Training Loss: 10075.010949414063; Average Validation Loss: 22698.722507911392\n",
      "Epoch 30 - Average Training Loss: 521.6159972839356; Average Validation Loss: 52.708351618126976\n",
      "Epoch 40 - Average Training Loss: 53122.6444796875; Average Validation Loss: 437794.19818037975\n",
      "Epoch 50 - Average Training Loss: 15161.722952539063; Average Validation Loss: 26877.08047369462\n",
      "Epoch 60 - Average Training Loss: 2318.119933203125; Average Validation Loss: 272.6395680874209\n",
      "Epoch 70 - Average Training Loss: 179.19639259643554; Average Validation Loss: 104.03220995166633\n",
      "Epoch 80 - Average Training Loss: 53924.9776234375; Average Validation Loss: 3944.0511242830303\n",
      "Epoch 90 - Average Training Loss: 11889.38573828125; Average Validation Loss: 9445.76426522943\n",
      "Epoch 100 - Average Training Loss: 47646305.221635155; Average Validation Loss: 146791.7654272152\n",
      "Epoch 110 - Average Training Loss: 21556.023881640624; Average Validation Loss: 2430.7582575158226\n",
      "Epoch 120 - Average Training Loss: 30793.64306484375; Average Validation Loss: 96970.5233386076\n",
      "Epoch 130 - Average Training Loss: 2248200.120275; Average Validation Loss: 1575971.805379747\n",
      "Epoch 140 - Average Training Loss: 22561.00885078125; Average Validation Loss: 10000.737143987342\n",
      "Epoch 149 - Average Training Loss: 5644.406424804687; Average Validation Loss: 4965.892559582674\n",
      "Learning Rate = 0.01; Dropout Rate = 0.2; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 3.2015709815979005; Average Validation Loss: 4.839540203915367\n",
      "Epoch 10 - Average Training Loss: 1.4192457812309265; Average Validation Loss: 15.118944638892065\n",
      "Epoch 20 - Average Training Loss: 1.2716024036407472; Average Validation Loss: 5.256709491150288\n",
      "Epoch 30 - Average Training Loss: 1.1911185508728028; Average Validation Loss: 4.22716437110418\n",
      "Epoch 40 - Average Training Loss: 1.1365912627220154; Average Validation Loss: 3.2109225128270404\n",
      "Epoch 50 - Average Training Loss: 1.120663293647766; Average Validation Loss: 3.0083283774460416\n",
      "Epoch 60 - Average Training Loss: 1.1001636359214784; Average Validation Loss: 4.341585645192786\n",
      "Epoch 70 - Average Training Loss: 1.0856788082122804; Average Validation Loss: 5.229589353633832\n",
      "Epoch 80 - Average Training Loss: 1.0698509155273437; Average Validation Loss: 3.409852580179142\n",
      "Epoch 90 - Average Training Loss: 1.0472505579948426; Average Validation Loss: 4.357506978360912\n",
      "Epoch 100 - Average Training Loss: 1.0282064984321595; Average Validation Loss: 3.008368567575382\n",
      "Epoch 110 - Average Training Loss: 1.0349685158729554; Average Validation Loss: 2.149523563022855\n",
      "Epoch 120 - Average Training Loss: 1.0459947407722474; Average Validation Loss: 2.605098196222812\n",
      "Epoch 130 - Average Training Loss: 1.0226894248008729; Average Validation Loss: 4.771380756474748\n",
      "Epoch 140 - Average Training Loss: 1.0235515162467956; Average Validation Loss: 5.170000118545339\n",
      "Epoch 149 - Average Training Loss: 1.0155459348678588; Average Validation Loss: 4.642796299125575\n",
      "Learning Rate = 0.01; Dropout Rate = 0.3; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 3.2015709815979005; Average Validation Loss: 4.839540203915367\n",
      "Epoch 10 - Average Training Loss: 1.4192457812309265; Average Validation Loss: 15.118944638892065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Average Training Loss: 1.2716024036407472; Average Validation Loss: 5.256709491150288\n",
      "Epoch 30 - Average Training Loss: 1.1911185508728028; Average Validation Loss: 4.22716437110418\n",
      "Epoch 40 - Average Training Loss: 1.1365912627220154; Average Validation Loss: 3.2109225128270404\n",
      "Epoch 50 - Average Training Loss: 1.120663293647766; Average Validation Loss: 3.0083283774460416\n",
      "Epoch 60 - Average Training Loss: 1.1001636359214784; Average Validation Loss: 4.341585645192786\n",
      "Epoch 70 - Average Training Loss: 1.0856788082122804; Average Validation Loss: 5.229589353633832\n",
      "Epoch 80 - Average Training Loss: 1.0698509155273437; Average Validation Loss: 3.409852580179142\n",
      "Epoch 90 - Average Training Loss: 1.0472505579948426; Average Validation Loss: 4.357506978360912\n",
      "Epoch 100 - Average Training Loss: 1.0282064984321595; Average Validation Loss: 3.008368567575382\n",
      "Epoch 110 - Average Training Loss: 1.0349685158729554; Average Validation Loss: 2.149523563022855\n",
      "Epoch 120 - Average Training Loss: 1.0459947407722474; Average Validation Loss: 2.605098196222812\n",
      "Epoch 130 - Average Training Loss: 1.0226894248008729; Average Validation Loss: 4.771380756474748\n",
      "Epoch 140 - Average Training Loss: 1.0235515162467956; Average Validation Loss: 5.170000118545339\n",
      "Epoch 149 - Average Training Loss: 1.0155459348678588; Average Validation Loss: 4.642796299125575\n",
      "Learning Rate = 0.01; Dropout Rate = 0.4; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 3.2015709815979005; Average Validation Loss: 4.839540203915367\n",
      "Epoch 10 - Average Training Loss: 1.4192457812309265; Average Validation Loss: 15.118944638892065\n",
      "Epoch 20 - Average Training Loss: 1.2716024036407472; Average Validation Loss: 5.256709491150288\n",
      "Epoch 30 - Average Training Loss: 1.1911185508728028; Average Validation Loss: 4.22716437110418\n",
      "Epoch 40 - Average Training Loss: 1.1365912627220154; Average Validation Loss: 3.2109225128270404\n",
      "Epoch 50 - Average Training Loss: 1.120663293647766; Average Validation Loss: 3.0083283774460416\n",
      "Epoch 60 - Average Training Loss: 1.1001636359214784; Average Validation Loss: 4.341585645192786\n",
      "Epoch 70 - Average Training Loss: 1.0856788082122804; Average Validation Loss: 5.229589353633832\n",
      "Epoch 80 - Average Training Loss: 1.0698509155273437; Average Validation Loss: 3.409852580179142\n",
      "Epoch 90 - Average Training Loss: 1.0472505579948426; Average Validation Loss: 4.357506978360912\n",
      "Epoch 100 - Average Training Loss: 1.0282064984321595; Average Validation Loss: 3.008368567575382\n",
      "Epoch 110 - Average Training Loss: 1.0349685158729554; Average Validation Loss: 2.149523563022855\n",
      "Epoch 120 - Average Training Loss: 1.0459947407722474; Average Validation Loss: 2.605098196222812\n",
      "Epoch 130 - Average Training Loss: 1.0226894248008729; Average Validation Loss: 4.771380756474748\n",
      "Epoch 140 - Average Training Loss: 1.0235515162467956; Average Validation Loss: 5.170000118545339\n",
      "Epoch 149 - Average Training Loss: 1.0155459348678588; Average Validation Loss: 4.642796299125575\n",
      "Learning Rate = 0.01; Dropout Rate = 0.5; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 3.2015709815979005; Average Validation Loss: 4.839540203915367\n",
      "Epoch 10 - Average Training Loss: 1.4192457812309265; Average Validation Loss: 15.118944638892065\n",
      "Epoch 20 - Average Training Loss: 1.2716024036407472; Average Validation Loss: 5.256709491150288\n",
      "Epoch 30 - Average Training Loss: 1.1911185508728028; Average Validation Loss: 4.22716437110418\n",
      "Epoch 40 - Average Training Loss: 1.1365912627220154; Average Validation Loss: 3.2109225128270404\n",
      "Epoch 50 - Average Training Loss: 1.120663293647766; Average Validation Loss: 3.0083283774460416\n",
      "Epoch 60 - Average Training Loss: 1.1001636359214784; Average Validation Loss: 4.341585645192786\n",
      "Epoch 70 - Average Training Loss: 1.0856788082122804; Average Validation Loss: 5.229589353633832\n",
      "Epoch 80 - Average Training Loss: 1.0698509155273437; Average Validation Loss: 3.409852580179142\n",
      "Epoch 90 - Average Training Loss: 1.0472505579948426; Average Validation Loss: 4.357506978360912\n",
      "Epoch 100 - Average Training Loss: 1.0282064984321595; Average Validation Loss: 3.008368567575382\n",
      "Epoch 110 - Average Training Loss: 1.0349685158729554; Average Validation Loss: 2.149523563022855\n",
      "Epoch 120 - Average Training Loss: 1.0459947407722474; Average Validation Loss: 2.605098196222812\n",
      "Epoch 130 - Average Training Loss: 1.0226894248008729; Average Validation Loss: 4.771380756474748\n",
      "Epoch 140 - Average Training Loss: 1.0235515162467956; Average Validation Loss: 5.170000118545339\n",
      "Epoch 149 - Average Training Loss: 1.0155459348678588; Average Validation Loss: 4.642796299125575\n",
      "Learning Rate = 0.001; Dropout Rate = 0.2; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 5.275111150932312; Average Validation Loss: 2.8600513603113873\n",
      "Epoch 10 - Average Training Loss: 1.319765265274048; Average Validation Loss: 5.345882783962201\n",
      "Epoch 20 - Average Training Loss: 1.2200471384048461; Average Validation Loss: 1.9275648065760165\n",
      "Epoch 30 - Average Training Loss: 1.1594544864654541; Average Validation Loss: 3.661548047126094\n",
      "Epoch 40 - Average Training Loss: 1.1305839431762694; Average Validation Loss: 5.441093366357345\n",
      "Epoch 50 - Average Training Loss: 1.0850940698623657; Average Validation Loss: 3.5252042420302767\n",
      "Epoch 60 - Average Training Loss: 1.065196255683899; Average Validation Loss: 2.493555192705951\n",
      "Epoch 70 - Average Training Loss: 1.0368893932342529; Average Validation Loss: 1.8120531130440627\n",
      "Epoch 80 - Average Training Loss: 1.0145046211242676; Average Validation Loss: 1.366158799280094\n",
      "Epoch 90 - Average Training Loss: 0.9937428332328796; Average Validation Loss: 1.8427228158033346\n",
      "Epoch 100 - Average Training Loss: 0.9855970410346985; Average Validation Loss: 2.423342876796481\n",
      "Epoch 110 - Average Training Loss: 0.9988724580764771; Average Validation Loss: 2.375453004354163\n",
      "Epoch 120 - Average Training Loss: 0.9798276980400086; Average Validation Loss: 1.8874955448923232\n",
      "Epoch 130 - Average Training Loss: 0.9645332220077515; Average Validation Loss: 2.357087289230733\n",
      "Epoch 140 - Average Training Loss: 0.9645358755111695; Average Validation Loss: 2.0543441168869596\n",
      "Epoch 149 - Average Training Loss: 0.9571588651657105; Average Validation Loss: 4.043570624122137\n",
      "Learning Rate = 0.001; Dropout Rate = 0.3; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 5.275111150932312; Average Validation Loss: 2.8600513603113873\n",
      "Epoch 10 - Average Training Loss: 1.319765265274048; Average Validation Loss: 5.345882783962201\n",
      "Epoch 20 - Average Training Loss: 1.2200471384048461; Average Validation Loss: 1.9275648065760165\n",
      "Epoch 30 - Average Training Loss: 1.1594544864654541; Average Validation Loss: 3.661548047126094\n",
      "Epoch 40 - Average Training Loss: 1.1305839431762694; Average Validation Loss: 5.441093366357345\n",
      "Epoch 50 - Average Training Loss: 1.0850940698623657; Average Validation Loss: 3.5252042420302767\n",
      "Epoch 60 - Average Training Loss: 1.065196255683899; Average Validation Loss: 2.493555192705951\n",
      "Epoch 70 - Average Training Loss: 1.0368893932342529; Average Validation Loss: 1.8120531130440627\n",
      "Epoch 80 - Average Training Loss: 1.0145046211242676; Average Validation Loss: 1.366158799280094\n",
      "Epoch 90 - Average Training Loss: 0.9937428332328796; Average Validation Loss: 1.8427228158033346\n",
      "Epoch 100 - Average Training Loss: 0.9855970410346985; Average Validation Loss: 2.423342876796481\n",
      "Epoch 110 - Average Training Loss: 0.9988724580764771; Average Validation Loss: 2.375453004354163\n",
      "Epoch 120 - Average Training Loss: 0.9798276980400086; Average Validation Loss: 1.8874955448923232\n",
      "Epoch 130 - Average Training Loss: 0.9645332220077515; Average Validation Loss: 2.357087289230733\n",
      "Epoch 140 - Average Training Loss: 0.9645358755111695; Average Validation Loss: 2.0543441168869596\n",
      "Epoch 149 - Average Training Loss: 0.9571588651657105; Average Validation Loss: 4.043570624122137\n",
      "Learning Rate = 0.001; Dropout Rate = 0.4; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 5.275111150932312; Average Validation Loss: 2.8600513603113873\n",
      "Epoch 10 - Average Training Loss: 1.319765265274048; Average Validation Loss: 5.345882783962201\n",
      "Epoch 20 - Average Training Loss: 1.2200471384048461; Average Validation Loss: 1.9275648065760165\n",
      "Epoch 30 - Average Training Loss: 1.1594544864654541; Average Validation Loss: 3.661548047126094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Average Training Loss: 1.1305839431762694; Average Validation Loss: 5.441093366357345\n",
      "Epoch 50 - Average Training Loss: 1.0850940698623657; Average Validation Loss: 3.5252042420302767\n",
      "Epoch 60 - Average Training Loss: 1.065196255683899; Average Validation Loss: 2.493555192705951\n",
      "Epoch 70 - Average Training Loss: 1.0368893932342529; Average Validation Loss: 1.8120531130440627\n",
      "Epoch 80 - Average Training Loss: 1.0145046211242676; Average Validation Loss: 1.366158799280094\n",
      "Epoch 90 - Average Training Loss: 0.9937428332328796; Average Validation Loss: 1.8427228158033346\n",
      "Epoch 100 - Average Training Loss: 0.9855970410346985; Average Validation Loss: 2.423342876796481\n",
      "Epoch 110 - Average Training Loss: 0.9988724580764771; Average Validation Loss: 2.375453004354163\n",
      "Epoch 120 - Average Training Loss: 0.9798276980400086; Average Validation Loss: 1.8874955448923232\n",
      "Epoch 130 - Average Training Loss: 0.9645332220077515; Average Validation Loss: 2.357087289230733\n",
      "Epoch 140 - Average Training Loss: 0.9645358755111695; Average Validation Loss: 2.0543441168869596\n",
      "Epoch 149 - Average Training Loss: 0.9571588651657105; Average Validation Loss: 4.043570624122137\n",
      "Learning Rate = 0.001; Dropout Rate = 0.5; Batch Size = 128\n",
      "Epoch 0 - Average Training Loss: 5.275111150932312; Average Validation Loss: 2.8600513603113873\n",
      "Epoch 10 - Average Training Loss: 1.319765265274048; Average Validation Loss: 5.345882783962201\n",
      "Epoch 20 - Average Training Loss: 1.2200471384048461; Average Validation Loss: 1.9275648065760165\n",
      "Epoch 30 - Average Training Loss: 1.1594544864654541; Average Validation Loss: 3.661548047126094\n",
      "Epoch 40 - Average Training Loss: 1.1305839431762694; Average Validation Loss: 5.441093366357345\n",
      "Epoch 50 - Average Training Loss: 1.0850940698623657; Average Validation Loss: 3.5252042420302767\n",
      "Epoch 60 - Average Training Loss: 1.065196255683899; Average Validation Loss: 2.493555192705951\n",
      "Epoch 70 - Average Training Loss: 1.0368893932342529; Average Validation Loss: 1.8120531130440627\n",
      "Epoch 80 - Average Training Loss: 1.0145046211242676; Average Validation Loss: 1.366158799280094\n",
      "Epoch 90 - Average Training Loss: 0.9937428332328796; Average Validation Loss: 1.8427228158033346\n",
      "Epoch 100 - Average Training Loss: 0.9855970410346985; Average Validation Loss: 2.423342876796481\n",
      "Epoch 110 - Average Training Loss: 0.9988724580764771; Average Validation Loss: 2.375453004354163\n",
      "Epoch 120 - Average Training Loss: 0.9798276980400086; Average Validation Loss: 1.8874955448923232\n",
      "Epoch 130 - Average Training Loss: 0.9645332220077515; Average Validation Loss: 2.357087289230733\n",
      "Epoch 140 - Average Training Loss: 0.9645358755111695; Average Validation Loss: 2.0543441168869596\n",
      "Epoch 149 - Average Training Loss: 0.9571588651657105; Average Validation Loss: 4.043570624122137\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader to handle batching\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "\n",
    "epochs = numEpochs\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "for lr, dropout_rate in product(learning_rates, dropout_rates):\n",
    "    torch.manual_seed(23)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = Model()\n",
    "    model.init_weights()\n",
    "    # Move the model to the GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    print(f'Learning Rate = {lr}; Dropout Rate = {dropout_rate}; Batch Size = {batch_size}')\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Move batch data to GPU\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_preds = model(batch_X)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(y_preds, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the total loss for the epoch\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        average_epoch_loss = epoch_loss / len(train_loader)\n",
    "        losses_train.append(average_epoch_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for batch_X_val, batch_y_val in val_loader:\n",
    "                # Move batch validation data to GPU\n",
    "                batch_X_val, batch_y_val = batch_X_val.to(device), batch_y_val.to(device)\n",
    "\n",
    "                # Forward pass for validation\n",
    "                y_preds_val = model(batch_X_val)\n",
    "\n",
    "                # Calculate the validation loss\n",
    "                loss_val = criterion(y_preds_val, batch_y_val)\n",
    "\n",
    "                # Accumulate the total validation loss for the epoch\n",
    "                val_loss += loss_val.item()\n",
    "\n",
    "        # Calculate average validation loss for the epoch\n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "        losses_val.append(average_val_loss)\n",
    "\n",
    "        # Print the average loss and validation loss every 10 epochs\n",
    "        if epoch % 10 == 0 or epoch == numEpochs-1:\n",
    "            print(f'Epoch {epoch} - Average Training Loss: {average_epoch_loss}; Average Validation Loss: {average_val_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_train = losses_train[:-1]\n",
    "# losses_val = losses_train[:-1]\n",
    "\n",
    "plt.plot(range(0, epochs, 10), losses_train, marker='o', linestyle='-', color='blue', label='Training')\n",
    "plt.plot(range(0, epochs, 10), losses_val, marker='o', linestyle='-', color='red', label='Validation')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9fa84f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming X_test and y_test are your test data and labels\n",
    "tolerance = 50\n",
    "correct = 0 \n",
    "incorrect = 0\n",
    "total = 0\n",
    "correct_x3 = 0\n",
    "correct_y3 = 0\n",
    "correct_z3 = 0\n",
    "correct_n3 = 0\n",
    "correct_sigma31 = 0\n",
    "correct_sigma32 = 0\n",
    "correct_sigma33 = 0\n",
    "\n",
    "incorrect_x3 = 0\n",
    "incorrect_y3 = 0\n",
    "incorrect_z3 = 0\n",
    "incorrect_n3 = 0\n",
    "incorrect_sigma31 = 0\n",
    "incorrect_sigma32 = 0\n",
    "incorrect_sigma33 = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        # Move data to the same device as the model\n",
    "        data = data.view(1, -1).to(device)\n",
    "        \n",
    "        # Forward pass without adding a batch dimension\n",
    "        y_val = model.forward(data)\n",
    "        \n",
    "        # Calculate tolerance ranges\n",
    "        for j in range(len(y_test[i])):\n",
    "            y_val_max_val = y_val[0][j] + tolerance\n",
    "            y_val_min_val = y_val[0][j] - tolerance\n",
    "            if y_test[i][j] <= y_val_max_val and y_test[i][j] >= y_val_min_val:\n",
    "                correct += 1\n",
    "                if j == 0:\n",
    "                    correct_x3 += 1\n",
    "                elif j == 1:\n",
    "                    correct_y3 += 1\n",
    "                elif j == 2:\n",
    "                    correct_z3 += 1\n",
    "                elif j == 3:\n",
    "                    correct_n3 += 1\n",
    "                elif j == 4:\n",
    "                    correct_sigma31 += 1\n",
    "                elif j == 5:\n",
    "                    correct_sigma32 += 1\n",
    "                elif j == 6:\n",
    "                    correct_sigma33 += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "                if j == 0:\n",
    "                    incorrect_x3 += 1\n",
    "                elif j == 1:\n",
    "                    incorrect_y3 += 1\n",
    "                elif j == 2:\n",
    "                    incorrect_z3 += 1\n",
    "                elif j == 3:\n",
    "                    incorrect_n3 += 1\n",
    "                elif j == 4:\n",
    "                    incorrect_sigma31 += 1\n",
    "                elif j == 5:\n",
    "                    incorrect_sigma32 += 1\n",
    "                elif j == 6:\n",
    "                    incorrect_sigma33 += 1\n",
    "            total += 1\n",
    "\n",
    "print(f'Total: {total}; Correct: {correct}; Incorrect: {incorrect}; Accuracy: {correct / total} - {(correct/total)*100}%')\n",
    "print(f'Correct Counts:')\n",
    "print(f'x3: {correct_x3} - Percent of Total: {(correct_x3 / total)*100}%')\n",
    "print(f'y3: {correct_y3} - Percent of Total: {(correct_y3 / total)*100}%')\n",
    "print(f'z3: {correct_z3} - Percent of Total: {(correct_z3 / total)*100}%')\n",
    "print(f'n3: {correct_n3} - Percent of Total: {(correct_n3 / total)*100}%')\n",
    "print(f'sigma31: {correct_sigma31} - Percent of Total: {(correct_sigma31 / total)*100}%')\n",
    "print(f'sigma32: {correct_sigma32} - Percent of Total: {(correct_sigma32 / total)*100}%')\n",
    "print(f'sigma33: {correct_sigma33} - Percent of Total: {(correct_sigma33 / total)*100}%')\n",
    "\n",
    "print(f'Incorrect Counts:')\n",
    "print(f'x3: {incorrect_x3} - Percent of Total: {(incorrect_x3 / total)*100}%')\n",
    "print(f'y3: {incorrect_y3} - Percent of Total: {(incorrect_y3 / total)*100}%')\n",
    "print(f'z3: {incorrect_z3} - Percent of Total: {(incorrect_z3 / total)*100}%')\n",
    "print(f'n3: {incorrect_n3} - Percent of Total: {(incorrect_n3 / total)*100}%')\n",
    "print(f'sigma31: {incorrect_sigma31} - Percent of Total: {(incorrect_sigma31 / total)*100}%')\n",
    "print(f'sigma32: {incorrect_sigma32} - Percent of Total: {(incorrect_sigma32 / total)*100}%')\n",
    "print(f'sigma33: {incorrect_sigma33} - Percent of Total: {(incorrect_sigma33 / total)*100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translational Error \n",
    "#et,i =(xi − xˆi)2+ (yi − yˆi)2+(z − zˆi)2\n",
    "\n",
    "total_count = 0\n",
    "total_et = 0 \n",
    "total_etheta = 0\n",
    "et = 0\n",
    "etheta_1 = 0\n",
    "etheta_2 = 0\n",
    "etheta = 0\n",
    "\n",
    "model.eval()\n",
    "# Move the model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        # Move data to the same device as the model\n",
    "        data = data.view(1, -1).to(device)\n",
    "        \n",
    "        # Forward pass without adding a batch dimension\n",
    "        y_val = model.forward(data)\n",
    "        \n",
    "        x3_pred = y_val[0][0]\n",
    "        y3_pred = y_val[0][1]\n",
    "        z3_pred = y_val[0][2]\n",
    "        n3_pred = y_val[0][3]\n",
    "        sigma31_pred = y_val[0][4]\n",
    "        sigma32_pred = y_val[0][5]\n",
    "        sigma33_pred = y_val[0][6]\n",
    "        \n",
    "        x3_real = y_test[i][0]\n",
    "        y3_real = y_test[i][1]\n",
    "        z3_real = y_test[i][2]\n",
    "        n3_real = y_test[i][3]\n",
    "        sigma31_real = y_test[i][4]\n",
    "        sigma32_real = y_test[i][5]\n",
    "        sigma33_real = y_test[i][6]\n",
    "        \n",
    "        n3_pred = n3_pred.cpu().numpy()\n",
    "        n3_real = n3_real.cpu().numpy()\n",
    "        sigma31_pred = sigma31_pred.cpu().numpy()\n",
    "        sigma31_real = sigma31_real.cpu().numpy()\n",
    "        sigma32_pred = sigma32_pred.cpu().numpy()\n",
    "        sigma32_real = sigma32_real.cpu().numpy()\n",
    "        sigma33_pred = sigma33_pred.cpu().numpy()\n",
    "        sigma33_real = sigma33_real.cpu().numpy()\n",
    "\n",
    "        et = (pow(x3_pred - x3_real, 2) +pow(y3_pred-y3_real, 2)+pow(z3_pred-z3_real, 2))**0.5\n",
    "        #print(f'Translational Error: {et}')\n",
    "        \n",
    "        #calculate etheta in degrees\n",
    "        etheta_1 = 2 * np.arccos(np.clip((n3_pred * n3_real) + (sigma31_pred * sigma31_real) + (sigma32_pred * sigma32_real) + (sigma33_pred * sigma33_real), -1, 1))\n",
    "        etheta_2 = 2 * np.arccos(np.clip((n3_pred * n3_real) - (sigma31_pred * sigma31_real) - (sigma32_pred * sigma32_real) - (sigma33_pred * sigma33_real), -1, 1))\n",
    "        etheta = min(etheta_1, etheta_2)\n",
    "\n",
    "        # Convert etheta to degrees \n",
    "        etheta_degrees = math.degrees(etheta)\n",
    "\n",
    "        #print(f'Rotational Error: {etheta} radians; {etheta_degrees} degrees; Etheta 1: {etheta_1}; Etheta 2: {etheta_2}')\n",
    "        total_et += et\n",
    "        total_etheta += etheta\n",
    "        total_count += 1\n",
    "        \n",
    "    print(f'Total Count: {total_count}; Average Translational Error (et) = {total_et / total_count} mm; {(total_et / total_count)/25.4} in') \n",
    "    print(f'Total Count: {total_count}; Average Rotational Error (etheta) = {total_etheta / total_count} radians; {math.degrees(total_etheta / total_count)} degrees') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph a sample of predicted vs. actual values for each variable\n",
    "x3_lst = []\n",
    "y3_lst = []\n",
    "z3_lst = []\n",
    "n3_lst = []\n",
    "sigma31_lst = []\n",
    "sigma32_lst = []\n",
    "sigma33_lst = []\n",
    "x3_lst_p = []\n",
    "y3_lst_p = []\n",
    "z3_lst_p = []\n",
    "n3_lst_p = []\n",
    "sigma31_lst_p = []\n",
    "sigma32_lst_p = []\n",
    "sigma33_lst_p = []\n",
    "\n",
    "sample_points = 10\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        # Move data to the same device as the model\n",
    "        data = data.view(1, -1).to(device)\n",
    "        \n",
    "        pred_vals = model.forward(data) \n",
    "\n",
    "        # Move the predictions back to CPU if necessary\n",
    "        pred_vals_np = pred_vals.cpu().numpy()\n",
    "        y_test_np = y_test[i].numpy()\n",
    "\n",
    "        # set the values in the proper list for graphing\n",
    "        for j in range(len(y_test_np)):\n",
    "            if j == 0:\n",
    "                x3_lst.append(y_test_np[j])\n",
    "                x3_lst_p.append(pred_vals_np[0, j])\n",
    "            elif j == 1:\n",
    "                y3_lst.append(y_test_np[j])\n",
    "                y3_lst_p.append(pred_vals_np[0, j])\n",
    "            elif j == 2:\n",
    "                z3_lst.append(y_test_np[j])\n",
    "                z3_lst_p.append(pred_vals_np[0, j])\n",
    "            elif j == 3:\n",
    "                n3_lst.append(y_test_np[j])\n",
    "                n3_lst_p.append(pred_vals_np[0, j])\n",
    "            elif j == 4:\n",
    "                sigma31_lst.append(y_test_np[j])\n",
    "                sigma31_lst_p.append(pred_vals_np[0, j])\n",
    "            elif j == 5:\n",
    "                sigma32_lst.append(y_test_np[j])\n",
    "                sigma32_lst_p.append(pred_vals_np[0, j])\n",
    "            elif j == 6:\n",
    "                sigma33_lst.append(y_test_np[j])\n",
    "                sigma33_lst_p.append(pred_vals_np[0, j])           \n",
    "        if i == sample_points:\n",
    "            break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58046cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_names = ['x3_lst', 'y3_lst', 'z3_lst', 'n3_lst', 'sigma31_lst', 'sigma32_lst', 'sigma33_lst']\n",
    "predictions_names = ['x3_lst_p', 'y3_lst_p', 'z3_lst_p', 'n3_lst_p', 'sigma31_lst_p', 'sigma32_lst_p', 'sigma33_lst_p']\n",
    "\n",
    "actual_data = [x3_lst, y3_lst, z3_lst, n3_lst, sigma31_lst, sigma32_lst, sigma33_lst]\n",
    "predictions_data = [x3_lst_p, y3_lst_p, z3_lst_p, n3_lst_p, sigma31_lst_p, sigma32_lst_p, sigma33_lst_p]\n",
    "\n",
    "num_rows = 3\n",
    "num_cols = 3\n",
    "\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Tolerance value\n",
    "tolerance = 14.71  \n",
    "\n",
    "\n",
    "# Loop through the plots and plot actual vs. predicted for each\n",
    "for i in range(min(num_rows * num_cols, len(actual_data))):\n",
    "    # Generate x-axis values (index values)\n",
    "    index_values = np.arange(len(actual_data[i]))\n",
    "\n",
    "    # Plotting the scatter plot for actual values\n",
    "    axs[i].scatter(index_values, actual_data[i], color='blue', label='Actual', marker='o')\n",
    "\n",
    "    # Plotting the scatter plot for predicted values\n",
    "    axs[i].scatter(index_values, predictions_data[i], color='red', label='Predicted', marker='x')\n",
    "\n",
    "    # Plotting points with tolerance above and below actual values\n",
    "    axs[i].errorbar(index_values, actual_data[i], yerr=tolerance, fmt='o', color='green', alpha=0.5)\n",
    "\n",
    "    #set y-axis limits\n",
    "    #axs[i].set_ylim([overall_min, overall_max])\n",
    "    \n",
    "    # Adding labels and title\n",
    "    axs[i].set_xlabel('Index')\n",
    "    axs[i].set_ylabel('Values')\n",
    "    axs[i].set_title(f'Plot {actual_names[i]}')\n",
    "\n",
    "    # Adding a legend\n",
    "    axs[i].legend()\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7b951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
