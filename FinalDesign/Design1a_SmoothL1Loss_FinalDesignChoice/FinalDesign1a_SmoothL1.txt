Architecture
class Model(nn.Module):
    def __init__(self, in_features=numInputs, h1=batch_size, h2=96, h3=50, h4=30, h5=25, h6=18, h7=15, out_features=numOutputs):
        super().__init__()
        self.fc1 = nn.Linear(in_features, h1)
        self.batchLayer = nn.BatchNorm1d(h1)
        self.layerNorm = LayerNorm(h1)
        self.fc2 = nn.Linear(h1, h2)
        self.fc3 = nn.Linear(h2, h3)
        self.dropout1 = nn.Dropout(0.2)
        self.fc4 = nn.Linear(h3, h4)
        self.fc5 = nn.Linear(h4, h5)
        self.dropout2 = nn.Dropout(0.2)
        self.fc6 = nn.Linear(h5, h6)
        self.fc7 = nn.Linear(h6, h7)
        self.out = nn.Linear(h7, out_features)

        # Apply He Initialization to the linear layers
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=0.2)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.PReLU):
                # Initialize PReLU parameters
                init.constant_(m.weight, 0.25)  # Set the negative slope to 0.25

    def forward(self, x):
        x=self.fc1(x)
        x = self.batchLayer(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  
        x = self.layerNorm(x)
        x=self.fc2(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  
        x=self.fc3(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  
        x = self.dropout1(x)
        x=self.fc4(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device)) 
        x=self.fc5(x)
        x = self.dropout2(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  
        x=self.fc6(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  
        x=self.fc7(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  
        x = self.out(x)
        
        return x

Batch Size 128

Epoch 0 - Average Training Loss: 5.310764192962647; Average Validation Loss: 7.509973544108717
Epoch 10 - Average Training Loss: 1.5205153030395508; Average Validation Loss: 7.397005467475215
Epoch 20 - Average Training Loss: 1.4249116876602173; Average Validation Loss: 4.814003920253319
Epoch 30 - Average Training Loss: 1.3701266653060913; Average Validation Loss: 2.8917684223078473
Epoch 40 - Average Training Loss: 1.345176550102234; Average Validation Loss: 2.832187368899961
Epoch 50 - Average Training Loss: 1.3416389625549316; Average Validation Loss: 8.444657422319244
Epoch 60 - Average Training Loss: 1.3174958829879762; Average Validation Loss: 6.855307965338985
Epoch 70 - Average Training Loss: 1.2929717735290527; Average Validation Loss: 3.475369441358349
Epoch 80 - Average Training Loss: 1.276271615600586; Average Validation Loss: 3.6098761769789682
Epoch 90 - Average Training Loss: 1.2689344356536865; Average Validation Loss: 3.9681937151317355
Epoch 100 - Average Training Loss: 1.2639189189910889; Average Validation Loss: 2.8482563012762916
Epoch 110 - Average Training Loss: 1.2983285182952882; Average Validation Loss: 2.9774161018902743
Epoch 120 - Average Training Loss: 1.2793501134872436; Average Validation Loss: 4.625607074061526
Epoch 130 - Average Training Loss: 1.2461008582115174; Average Validation Loss: 2.486721041836316
Epoch 140 - Average Training Loss: 1.2364050769805908; Average Validation Loss: 2.2801974544042274
Epoch 149 - Average Training Loss: 1.2419992730140685; Average Validation Loss: 2.3887596311448496

Tolerance - 1.0
Total: 70000; Correct: 43372; Incorrect: 26628; Accuracy: 0.6196 - 61.96%
Correct Counts:
x3: 1706 - Percent of Total: 2.4371428571428573%
y3: 796 - Percent of Total: 1.1371428571428572%
z3: 870 - Percent of Total: 1.2428571428571429%
n3: 10000 - Percent of Total: 14.285714285714285%
sigma31: 10000 - Percent of Total: 14.285714285714285%
sigma32: 10000 - Percent of Total: 14.285714285714285%
sigma33: 10000 - Percent of Total: 14.285714285714285%
Incorrect Counts:
x3: 8294 - Percent of Total: 11.848571428571429%
y3: 9204 - Percent of Total: 13.14857142857143%
z3: 9130 - Percent of Total: 13.042857142857143%
n3: 0 - Percent of Total: 0.0%
sigma31: 0 - Percent of Total: 0.0%
sigma32: 0 - Percent of Total: 0.0%
sigma33: 0 - Percent of Total: 0.0%

Tolerance - 2.5
Total: 70000; Correct: 48528; Incorrect: 21472; Accuracy: 0.6932571428571429 - 69.32571428571428%
Correct Counts:
x3: 4414 - Percent of Total: 6.305714285714285%
y3: 1959 - Percent of Total: 2.798571428571429%
z3: 2155 - Percent of Total: 3.0785714285714283%
n3: 10000 - Percent of Total: 14.285714285714285%
sigma31: 10000 - Percent of Total: 14.285714285714285%
sigma32: 10000 - Percent of Total: 14.285714285714285%
sigma33: 10000 - Percent of Total: 14.285714285714285%
Incorrect Counts:
x3: 5586 - Percent of Total: 7.9799999999999995%
y3: 8041 - Percent of Total: 11.487142857142857%
z3: 7845 - Percent of Total: 11.207142857142857%
n3: 0 - Percent of Total: 0.0%
sigma31: 0 - Percent of Total: 0.0%
sigma32: 0 - Percent of Total: 0.0%
sigma33: 0 - Percent of Total: 0.0%

Tolerance - 5.0
Total: 70000; Correct: 56249; Incorrect: 13751; Accuracy: 0.8035571428571429 - 80.35571428571428%
Correct Counts:
x3: 8095 - Percent of Total: 11.564285714285715%
y3: 4113 - Percent of Total: 5.8757142857142854%
z3: 4041 - Percent of Total: 5.772857142857143%
n3: 10000 - Percent of Total: 14.285714285714285%
sigma31: 10000 - Percent of Total: 14.285714285714285%
sigma32: 10000 - Percent of Total: 14.285714285714285%
sigma33: 10000 - Percent of Total: 14.285714285714285%
Incorrect Counts:
x3: 1905 - Percent of Total: 2.7214285714285715%
y3: 5887 - Percent of Total: 8.41%
z3: 5959 - Percent of Total: 8.512857142857143%
n3: 0 - Percent of Total: 0.0%
sigma31: 0 - Percent of Total: 0.0%
sigma32: 0 - Percent of Total: 0.0%
sigma33: 0 - Percent of Total: 0.0%

Tolerance - 14.71
Total: 70000; Correct: 67532; Incorrect: 2468; Accuracy: 0.9647428571428571 - 96.47428571428571%
Correct Counts:
x3: 9997 - Percent of Total: 14.281428571428572%
y3: 8801 - Percent of Total: 12.572857142857144%
z3: 8734 - Percent of Total: 12.477142857142857%
n3: 10000 - Percent of Total: 14.285714285714285%
sigma31: 10000 - Percent of Total: 14.285714285714285%
sigma32: 10000 - Percent of Total: 14.285714285714285%
sigma33: 10000 - Percent of Total: 14.285714285714285%
Incorrect Counts:
x3: 3 - Percent of Total: 0.004285714285714286%
y3: 1199 - Percent of Total: 1.7128571428571426%
z3: 1266 - Percent of Total: 1.8085714285714285%
n3: 0 - Percent of Total: 0.0%
sigma31: 0 - Percent of Total: 0.0%
sigma32: 0 - Percent of Total: 0.0%
sigma33: 0 - Percent of Total: 0.0%

Tolerance - 50
Total: 70000; Correct: 70000; Incorrect: 0; Accuracy: 1.0 - 100.0%
Correct Counts:
x3: 10000 - Percent of Total: 14.285714285714285%
y3: 10000 - Percent of Total: 14.285714285714285%
z3: 10000 - Percent of Total: 14.285714285714285%
n3: 10000 - Percent of Total: 14.285714285714285%
sigma31: 10000 - Percent of Total: 14.285714285714285%
sigma32: 10000 - Percent of Total: 14.285714285714285%
sigma33: 10000 - Percent of Total: 14.285714285714285%
Incorrect Counts:
x3: 0 - Percent of Total: 0.0%
y3: 0 - Percent of Total: 0.0%
z3: 0 - Percent of Total: 0.0%
n3: 0 - Percent of Total: 0.0%
sigma31: 0 - Percent of Total: 0.0%
sigma32: 0 - Percent of Total: 0.0%
sigma33: 0 - Percent of Total: 0.0%



Translational and Rotational Error:
Total Count: 10000; Average Translational Error (et) = 12.447391510009766 mm; 0.49005478620529175 in
Total Count: 10000; Average Rotational Error (etheta) = 0.18862890834171536 radians; 10.807640342140338 degrees
