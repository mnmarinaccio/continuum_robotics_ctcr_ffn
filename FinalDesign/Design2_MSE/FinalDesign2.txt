Architecture
class Model(nn.Module):
    def __init__(self, in_features=numInputs, h1=batch_size, h2=96, h3=50, h4=30, h5=25, h6=18, h7=15,
                 h8=96, h9=50, h10=30, h11=25, h12=18, h13=15, out_features=numOutputs):
        super().__init__()
        self.fc1 = nn.Linear(in_features, h1)
        self.batchLayer = nn.BatchNorm1d(h1)
        self.layerNorm = LayerNorm(h1)
        self.fc2 = nn.Linear(h1, h2)
        self.fc3 = nn.Linear(h2, h3)
        self.dropout1 = nn.Dropout(0.2)
        self.fc4 = nn.Linear(h3, h4)
        self.fc5 = nn.Linear(h4, h5)
        self.dropout2 = nn.Dropout(0.2)
        self.fc6 = nn.Linear(h5, h6)
        self.fc7 = nn.Linear(h6, h7)
        self.dropout3 = nn.Dropout(0.2)
        self.fc8 = nn.Linear(h7, h8)
        self.fc9 = nn.Linear(h8, h9)
        self.dropout4 = nn.Dropout(0.2)
        self.fc10 = nn.Linear(h9, h10)
        self.fc11 = nn.Linear(h10, h11)
        self.dropout5 = nn.Dropout(0.2)
        self.fc12 = nn.Linear(h11, h12)
        self.fc13 = nn.Linear(h12, h13)
        self.out = nn.Linear(h13, out_features)

        # Apply He Initialization to the linear layers
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=0.2)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.PReLU):
                # Initialize PReLU parameters
                init.constant_(m.weight, 0.25)  # Set the negative slope to 0.25

    def forward(self, x):
        x=self.fc1(x)
        x = self.batchLayer(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  
        x = self.layerNorm(x)
        x=self.fc2(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  
        x=self.fc3(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  
        x = self.dropout1(x)
        x=self.fc4(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device)) 
        x=self.fc5(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device)) 
        x = self.dropout2(x)
        x=self.fc6(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))  
        x=self.fc7(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))
        x = self.dropout3(x)
        x=self.fc8(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))
        x=self.fc9(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))
        x = self.dropout4(x)
        x=self.fc10(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))
        x=self.fc11(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))
        x = self.dropout5(x)
        x=self.fc12(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))
        x=self.fc13(x)
        x = F.prelu(x, weight=torch.Tensor([0.25]).to(x.device))
        x = self.out(x)
        
        return x

Epoch 0 - Average Training Loss: 236.97421909179687; Average Validation Loss: 462.17023014116893
Epoch 10 - Average Training Loss: 18.996308203125; Average Validation Loss: 75.23761092560201
Epoch 20 - Average Training Loss: 15.60241121673584; Average Validation Loss: 157.15465516681914
Epoch 30 - Average Training Loss: 14.03360008392334; Average Validation Loss: 108.04304620284069
Epoch 40 - Average Training Loss: 13.7217428314209; Average Validation Loss: 100.68418700785577
Epoch 50 - Average Training Loss: 12.963477464294433; Average Validation Loss: 67.58117506775675
Epoch 60 - Average Training Loss: 12.992276695251466; Average Validation Loss: 36.052402448050586
Epoch 70 - Average Training Loss: 12.921149090576172; Average Validation Loss: 72.68394827540916
Epoch 80 - Average Training Loss: 12.46239683380127; Average Validation Loss: 60.059936571724805
Epoch 90 - Average Training Loss: 12.63564666595459; Average Validation Loss: 54.489526338215114
Epoch 100 - Average Training Loss: 12.384142828369141; Average Validation Loss: 84.36034818238849
Epoch 110 - Average Training Loss: 12.397063320922852; Average Validation Loss: 55.97592192058322
Epoch 120 - Average Training Loss: 12.439263931274414; Average Validation Loss: 67.24740383293056
Epoch 130 - Average Training Loss: 11.913395049285889; Average Validation Loss: 35.04332875601853
Epoch 140 - Average Training Loss: 11.643547789764405; Average Validation Loss: 54.497666129583045
Epoch 149 - Average Training Loss: 11.83105258026123; Average Validation Loss: 73.06991992419279

Tolerance - 1.0
Total: 70000; Correct: 41963; Incorrect: 28037; Accuracy: 0.5994714285714285 - 59.94714285714286%
Correct Counts:
x3: 769 - Percent of Total: 1.0985714285714285%
y3: 759 - Percent of Total: 1.0842857142857143%
z3: 435 - Percent of Total: 0.6214285714285714%
n3: 10000 - Percent of Total: 14.285714285714285%
sigma31: 10000 - Percent of Total: 14.285714285714285%
sigma32: 10000 - Percent of Total: 14.285714285714285%
sigma33: 10000 - Percent of Total: 14.285714285714285%
Incorrect Counts:
x3: 9231 - Percent of Total: 13.187142857142858%
y3: 9241 - Percent of Total: 13.201428571428572%
z3: 9565 - Percent of Total: 13.664285714285715%
n3: 0 - Percent of Total: 0.0%
sigma31: 0 - Percent of Total: 0.0%
sigma32: 0 - Percent of Total: 0.0%
sigma33: 0 - Percent of Total: 0.0%


Tolerance - 2.5
Total: 70000; Correct: 44834; Incorrect: 25166; Accuracy: 0.6404857142857143 - 64.04857142857144%
Correct Counts:
x3: 1876 - Percent of Total: 2.68%
y3: 1887 - Percent of Total: 2.6957142857142857%
z3: 1071 - Percent of Total: 1.53%
n3: 10000 - Percent of Total: 14.285714285714285%
sigma31: 10000 - Percent of Total: 14.285714285714285%
sigma32: 10000 - Percent of Total: 14.285714285714285%
sigma33: 10000 - Percent of Total: 14.285714285714285%
Incorrect Counts:
x3: 8124 - Percent of Total: 11.605714285714285%
y3: 8113 - Percent of Total: 11.59%
z3: 8929 - Percent of Total: 12.755714285714287%
n3: 0 - Percent of Total: 0.0%
sigma31: 0 - Percent of Total: 0.0%
sigma32: 0 - Percent of Total: 0.0%
sigma33: 0 - Percent of Total: 0.0%


Tolerance - 5.0
Total: 70000; Correct: 49559; Incorrect: 20441; Accuracy: 0.7079857142857143 - 70.79857142857144%
Correct Counts:
x3: 3757 - Percent of Total: 5.367142857142857%
y3: 3727 - Percent of Total: 5.324285714285714%
z3: 2075 - Percent of Total: 2.9642857142857144%
n3: 10000 - Percent of Total: 14.285714285714285%
sigma31: 10000 - Percent of Total: 14.285714285714285%
sigma32: 10000 - Percent of Total: 14.285714285714285%
sigma33: 10000 - Percent of Total: 14.285714285714285%
Incorrect Counts:
x3: 6243 - Percent of Total: 8.918571428571429%
y3: 6273 - Percent of Total: 8.961428571428572%
z3: 7925 - Percent of Total: 11.321428571428571%
n3: 0 - Percent of Total: 0.0%
sigma31: 0 - Percent of Total: 0.0%
sigma32: 0 - Percent of Total: 0.0%
sigma33: 0 - Percent of Total: 0.0%


Tolerance - 14.71
Total: 70000; Correct: 61894; Incorrect: 8106; Accuracy: 0.8842 - 88.42%
Correct Counts:
x3: 8215 - Percent of Total: 11.735714285714286%
y3: 8087 - Percent of Total: 11.552857142857142%
z3: 5592 - Percent of Total: 7.988571428571428%
n3: 10000 - Percent of Total: 14.285714285714285%
sigma31: 10000 - Percent of Total: 14.285714285714285%
sigma32: 10000 - Percent of Total: 14.285714285714285%
sigma33: 10000 - Percent of Total: 14.285714285714285%
Incorrect Counts:
x3: 1785 - Percent of Total: 2.55%
y3: 1913 - Percent of Total: 2.7328571428571427%
z3: 4408 - Percent of Total: 6.297142857142857%
n3: 0 - Percent of Total: 0.0%
sigma31: 0 - Percent of Total: 0.0%
sigma32: 0 - Percent of Total: 0.0%
sigma33: 0 - Percent of Total: 0.0%

Tolerance - 50.0
Total: 70000; Correct: 70000; Incorrect: 0; Accuracy: 1.0 - 100.0%
Correct Counts:
x3: 10000 - Percent of Total: 14.285714285714285%
y3: 10000 - Percent of Total: 14.285714285714285%
z3: 10000 - Percent of Total: 14.285714285714285%
n3: 10000 - Percent of Total: 14.285714285714285%
sigma31: 10000 - Percent of Total: 14.285714285714285%
sigma32: 10000 - Percent of Total: 14.285714285714285%
sigma33: 10000 - Percent of Total: 14.285714285714285%
Incorrect Counts:
x3: 0 - Percent of Total: 0.0%
y3: 0 - Percent of Total: 0.0%
z3: 0 - Percent of Total: 0.0%
n3: 0 - Percent of Total: 0.0%
sigma31: 0 - Percent of Total: 0.0%
sigma32: 0 - Percent of Total: 0.0%
sigma33: 0 - Percent of Total: 0.0%

Translational and Rotational Error
Total Count: 10000; Average Translational Error (et) = 20.74930763244629 mm; 0.8169018626213074 in
Total Count: 10000; Average Rotational Error (etheta) = 0.2603965449692092 radians; 14.919623026524237 degrees